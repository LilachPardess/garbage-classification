{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5338f1f5",
   "metadata": {},
   "source": [
    "# Homework 2: Multiclass Classification with PyTorch\n",
    "\n",
    "In this assignment, you will build, train, and evaluate a neural network for multiclass classification using PyTorch.\n",
    "You will use the [Garbage dataset](https://www.kaggle.com/datasets/mostafaabla/garbage-classification).\n",
    "The goal is to gain hands-on experience with:\n",
    "- Dataset preparation \n",
    "- Building two  PyTorch models\n",
    "- Loss functions for multiclass\n",
    "- Training loop and evaluation\n",
    "- Visualize of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf82d3",
   "metadata": {},
   "source": [
    "## About Dataset\n",
    "### Context\n",
    "This dataset has 15,150 images from 12 different classes of household garbage; paper, cardboard, biological, metal, plastic, green-glass, brown-glass, white-glass, clothes, shoes, batteries, and trash.\n",
    "\n",
    "Garbage Recycling is a key aspect of preserving our environment. To make the recycling process possible/easier, the garbage must be sorted to groups that have similar recycling process. I found that most available data sets classify garbage into a few classes (2 to 6 classes at most). Having the ability to sort the household garbage into more classes can result in dramatically increasing the percentage of the recycled garbage.\n",
    "\n",
    "### Content\n",
    "An ideal setting for data collection would be to place a camera above a conveyor where the garbage comes one by one, so that the camera can capture real garbage images. But since such a setup is not feasible at the moment I collected most of the images in this dataset by web scraping, I tried to get images close to garbage images whenever possible, for example in biological garbage category I searched for rotten vegetables, rotten fruits and food remains, etc. However, for some classes such as clothes or shoes it was more difficult to get images of clothes or shoes from the garbage, so mostly it was images of normal clothes. Nevertheless, being able to classify the images of this data set to 12 classes can be a big step towards improving the recycling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f150469",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpardess\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e7b169af90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import kagglehub\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488ee02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fef162a",
   "metadata": {},
   "source": [
    "### Dwonload and prepare dataset from kagglehub\n",
    "`kagglehub.dataset_download` downloads and extracts Kaggle datasets to a local cache directory (usually under `~/.cache/kagglehub/datasets/`). It returns the path to the unzipped dataset, preserving the original folder structure as found on Kaggle, such as one subfolder per class for image datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is the structure of the downloaded content?**\n",
    "\n",
    "* Inside the returned directory (`path`), you will find the files and folders as originally organized on Kaggle.\n",
    "* For the **garbage classification** dataset, you typically get a folder like:\n",
    "\n",
    "  ```\n",
    "  garbage_classification/\n",
    "      cardboard/\n",
    "      glass/\n",
    "      metal/\n",
    "      paper/\n",
    "      plastic/\n",
    "      trash/\n",
    "      ...\n",
    "  ```\n",
    "\n",
    "  Each subfolder contains images belonging to that class (a classic structure for use with `torchvision.datasets.ImageFolder`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1087d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image transform: Compose(\n",
      "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Download the latest version of the Kaggle dataset to a local directory\n",
    "path = kagglehub.dataset_download(\"mostafaabla/garbage-classification\",)\n",
    "\n",
    "# Set the data directory to the location of the downloaded images\n",
    "data_dir = os.path.join(path, \"garbage_classification\")\n",
    "\n",
    "# Set the desired image size for resizing\n",
    "img_size = 64\n",
    "\n",
    "# Define the image transformations to apply to each image:\n",
    "# - Resize the image to (img_size, img_size)\n",
    "# - Convert the image to a PyTorch tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),  # Resize images\n",
    "    transforms.ToTensor()                     # Convert to tensor\n",
    "])  \n",
    "\n",
    "print(f\"image transform: {transform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed197d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpardess\\.cache\\kagglehub\\datasets\\mostafaabla\\garbage-classification\\versions\\1\\garbage_classification\n",
      "Number of images: 15515 number of classes: 12\n",
      "Class names: ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)\n",
    "dataset  = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "classes_names = dataset.classes\n",
    "\n",
    "# dataset = ImageFolder(data_dir + '/Training', transform=transform)\n",
    "print(\"Number of images:\", len(dataset), \"number of classes:\", len(classes_names))\n",
    "print(\"Class names:\", classes_names)\n",
    "\n",
    "#fig, axes = plt.subplots(4, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f0e1e",
   "metadata": {},
   "source": [
    "### TODO 1:\n",
    "Create a 4×3 subplot that displays one example image from each category in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your coimport matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_images(dataset):\n",
    "    fig, axes = plt.subplots(4, 3)\n",
    "    axes = axes.flatten() \n",
    "\n",
    "    shown = set()\n",
    "\n",
    "    for idx, class_name in enumerate(dataset.classes):\n",
    "        for img, label in dataset: # img is of size torch.Size([3, 64, 64])\n",
    "            if dataset.classes[label] == class_name and class_name not in shown:\n",
    "                axes[idx].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "                axes[idx].set_title(class_name)\n",
    "                axes[idx].axis('off')\n",
    "                shown.add(class_name)\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#TODO 1 delete\n",
    "def show_image(dataset, index):\n",
    "    fig, axes = plt.subplots(1, 1)\n",
    "\n",
    "    img, lable = dataset[index];\n",
    "    axes.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    axes.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68525755",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2aba79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e2da23",
   "metadata": {},
   "source": [
    "### TODO 2:\n",
    "Shuffle the dataset and split it into training and validation sets, using 80% of the samples for training and 20% for validation. Make sure that the class distribution is preserved as much as possible in both splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15408b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def spit_dataset(dataset):\n",
    "    dataset_size = len(dataset) \n",
    "    # Create train/test split (80/20)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    test_size = len(dataset) - train_size\n",
    "    indices = torch.randperm(dataset_size)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def validate_distribution(dataset, title):\n",
    "    #TODO 1 imp\n",
    "    return;\n",
    "\n",
    "train_dataset, test_dataset = spit_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40025d8",
   "metadata": {},
   "source": [
    "### TODO 3:\n",
    "Visualize the class distribution in both the training and validation sets using a bar plot, so you can compare how well the splits represent the overall dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f391e0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     plt.tight_layout()\n\u001b[32m     41\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mplot_class_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mplot_class_distribution\u001b[39m\u001b[34m(train_dataset, test_dataset)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_class_distribution\u001b[39m(train_dataset, test_dataset):\n\u001b[32m     19\u001b[39m     num_classes = \u001b[38;5;28mlen\u001b[39m(classes_names)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     train_counts = \u001b[43mget_class_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     val_counts = get_class_counts(test_dataset)\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Convert counts to relative frequencies\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mget_class_counts\u001b[39m\u001b[34m(subset)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_class_counts\u001b[39m(subset):\n\u001b[32m     13\u001b[39m     class_sizes = [\u001b[32m0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(classes_names)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m class_sizes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataset.py:412\u001b[39m, in \u001b[36mSubset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset[[\u001b[38;5;28mself\u001b[39m.indices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\datasets\\folder.py:262\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> Image.Image:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    263\u001b[39m         img = Image.open(f)\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def show_distribution(subset, set_name):\n",
    "    print(\"Distuction of\", set_name, \": \")\n",
    "    class_sizes = [0] * len(classes_names)\n",
    "    for img, label in subset:\n",
    "        class_sizes[label] += 1\n",
    "\n",
    "    for idx in range(len(classes_names)):\n",
    "        percent = 100 * class_sizes[idx] / len(subset)\n",
    "        print(f\"Class '{classes_names[idx]}': {class_sizes[idx]} images ({percent:.2f}%)\")\n",
    "\n",
    "def get_class_counts(subset):\n",
    "    class_sizes = [0] * len(classes_names)\n",
    "    for img, label in subset:\n",
    "        class_sizes[label] += 1\n",
    "    return class_sizes\n",
    "\n",
    "def plot_class_distribution(train_dataset, test_dataset):\n",
    "    num_classes = len(classes_names)\n",
    "    train_counts = get_class_counts(train_dataset)\n",
    "    val_counts = get_class_counts(test_dataset)\n",
    "\n",
    "    # Convert counts to relative frequencies\n",
    "    train_total = sum(train_counts)\n",
    "    val_total = sum(val_counts)\n",
    "    train_rel = [count / train_total for count in train_counts]\n",
    "    val_rel = [count / val_total for count in val_counts]\n",
    "\n",
    "    x = range(num_classes)\n",
    "    width = 0.35  # width of the bars\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x, train_rel, width=width, label='Train')\n",
    "    plt.bar([i + width for i in x], val_rel, width=width, label='Validation')\n",
    "    plt.xticks([i + width/2 for i in x], classes_names, rotation=45)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Relative amount')\n",
    "    plt.title('Relative Class Distribution in Train and Validation Sets')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plot_class_distribution(train_dataset=train_dataset, test_dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645449fc",
   "metadata": {},
   "source": [
    "### TODO 4:\n",
    "Ensure that no single category accounts for more than 15% of the samples in the training set. If necessary, downsample the dominant classes. Then, visualize the new class distribution in the training set using a bar plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9a21a",
   "metadata": {},
   "source": [
    "### TODO 4:\n",
    "Ensure that no single category accounts for more than 15% of the samples in the training set. If necessary, downsample the dominant classes. Then, visualize the new class distribution in the training set using a bar plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import random\n",
    "\n",
    "max_clothes = int(0.15 * len(train_dataset))\n",
    "clothes_class_idx = classes_names.index(\"clothes\")\n",
    "clothes_indices = [i for i, idx in enumerate(train_dataset.indices)\n",
    "                   if dataset.samples[idx][1] == clothes_class_idx]\n",
    "\n",
    "random.seed(42) \n",
    "selected_clothes_indices = random.sample(clothes_indices, max_clothes)\n",
    "other_indices = [i for i in range(len(train_dataset.indices)) if i not in clothes_indices]\n",
    "final_indices = [train_dataset.indices[i] for i in other_indices + selected_clothes_indices]\n",
    "\n",
    "train_dataset = Subset(dataset, final_indices)\n",
    "\n",
    "#plot_class_distribution(train_dataset=train_dataset, test_dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201f8e1",
   "metadata": {},
   "source": [
    "## Implementation of Regularization Layers\n",
    "\n",
    "Implement two regularization layers from scratch:\n",
    "1. `BatchNorm2d`\n",
    "2. `LayerNorm`\n",
    "\n",
    "Make sure that all trainable parameters (such as scale and shift) are properly registered as part of the computational graph, so they are optimized during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8360fb",
   "metadata": {},
   "source": [
    "### TODO 5:\n",
    "Implement the BatchNorm2d layer from scratch using only basic PyTorch components (such as `nn.Module` and tensor operations), without relying on `nn.BatchNorm2d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f193d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features  # Number of feature channels (C in NCHW)\n",
    "        self.eps = eps  # Small value to avoid division by zero when normalizing\n",
    "        self.momentum = momentum  # Controls how quickly running stats are updated\n",
    "\n",
    "        # Learnable scale (gamma) and shift (beta) parameters, one per channel.\n",
    "        # These allow the network to undo normalization if needed, preserving representational power.\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "        # Running mean and variance are used during inference (model.eval()).\n",
    "        # They accumulate a moving average of the mean/variance seen during training.\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N, C, H, W) - batch, channels, height, width\n",
    "\n",
    "        if self.training:\n",
    "            # During training, compute mean and variance for each channel over the batch and spatial dims.\n",
    "            # This normalizes each feature map (channel) so that its output has mean 0 and variance 1.\n",
    "            mean = x.mean(dim=(0, 2, 3), keepdim=True)  # Shape: (1, C, 1, 1)\n",
    "            var = x.var(dim=(0, 2, 3), unbiased=False, keepdim=True)  # Shape: (1, C, 1, 1)\n",
    "\n",
    "            # Update running statistics for use during inference.\n",
    "            # This helps the model make consistent predictions on unseen data (test/production).\n",
    "            # Detach is used so that these updates are not part of the gradient computation.\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.view(-1).detach()\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.view(-1).detach()\n",
    "        else:\n",
    "            # During inference, use the running mean/var instead of batch statistics.\n",
    "            # This ensures deterministic output and avoids dependence on batch size.\n",
    "            mean = self.running_mean.view(1, -1, 1, 1)\n",
    "            var = self.running_var.view(1, -1, 1, 1)\n",
    "\n",
    "        # Normalize: subtract mean and divide by stddev (add eps for stability).\n",
    "        # This reduces internal covariate shift, making training faster and more stable.\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # Scale and shift: allow the network to learn optimal mean/variance for each channel.\n",
    "        # This step ensures the transformation can represent the identity function if needed.\n",
    "        out = self.gamma.view(1, -1, 1, 1) * x_hat + self.beta.view(1, -1, 1, 1)\n",
    "\n",
    "        # The output is normalized, but can be rescaled and shifted as needed for learning.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72548b",
   "metadata": {},
   "source": [
    "### TODO 6:\n",
    "Implement the LayerNorm layer from scratch using only basic PyTorch components (such as `nn.Module` and tensor operations), without relying on `nn.LayerNorm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            normalized_shape: input shape from an expected input of size\n",
    "                [*, normalized_shape[0], normalized_shape[1], ..., normalized_shape[-1]]\n",
    "                Typically, for images, this is the number of channels (C), height (H), and width (W).\n",
    "            eps: a value added to the denominator for numerical stability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Store the shape over which normalization is performed\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = tuple(normalized_shape)\n",
    "        self.eps = eps\n",
    "\n",
    "        # Learnable parameters for scaling (gamma) and shifting (beta)\n",
    "        # These allow the network to undo normalization if needed\n",
    "        self.gamma = nn.Parameter(torch.ones(self.normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute mean and variance over the last N dimensions (those in normalized_shape)\n",
    "        # This is different from BatchNorm, which normalizes over batch and spatial dims\n",
    "        dims = tuple(-i for i in range(1, len(self.normalized_shape)+1))\n",
    "        mean = x.mean(dim=dims, keepdim=True)  # Mean over normalized_shape\n",
    "        var = x.var(dim=dims, unbiased=False, keepdim=True)  # Variance over normalized_shape\n",
    "\n",
    "        # Normalize: subtract mean and divide by stddev (add eps for stability)\n",
    "        # This ensures each sample (not batch) has zero mean and unit variance over the normalized_shape\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # Scale and shift: apply learnable gamma and beta\n",
    "        # Broadcasting ensures gamma and beta are applied to the correct dimensions\n",
    "        out = self.gamma * x_hat + self.beta\n",
    "\n",
    "        # Output is normalized per sample, not per batch, which helps with stability\n",
    "        # especially in recurrent networks or when batch sizes are small\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd3273",
   "metadata": {},
   "source": [
    "## Traning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855054a3",
   "metadata": {},
   "source": [
    "### TODO 7:\n",
    "Complete the `GarbageClassifier` neural network by designing and implementing an architecture of your choice.  \n",
    "Make use of the provided `_block` and `_block_mp` building blocks as you see fit.  \n",
    "Allow the regularization type (e.g., `BatchNorm2d` or `LayerNorm`) to be specified from outside the class, so you can later compare the results between the two types of regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, norm_layer):\n",
    "        # Fix: Remove norm_layer from super() call\n",
    "        super(GarbageClassifier, self).__init__()\n",
    "        \n",
    "        # Set default normalization layer if none provided\n",
    "        if norm_layer is None:\n",
    "            norm_layer = MyBatchNorm2d \n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            self._block_mp(3, 96, kernel_size=11, stride=4, padding=0, norm_layer=norm_layer),\n",
    "            self._block_mp(96, 256, kernel_size=5, stride=1, padding=2, norm_layer=norm_layer),\n",
    "            self._block(256, 384, kernel_size=3, stride=1, padding=1, norm_layer=norm_layer),\n",
    "            self._block(384, 384, kernel_size=3, stride=1, padding=1, norm_layer=norm_layer),\n",
    "            self._block_mp(384, 256, kernel_size=3, stride=1, padding=1, norm_layer=norm_layer),\n",
    "        )   \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 1 * 1, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "       \n",
    "    def _block_mp(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, norm_layer=None, kernel_size_mp=2):\n",
    "        \"\"\"AlexNet style block with max pooling\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            norm_layer(out_channels), # Normalization layer that you have implemented\n",
    "            nn.MaxPool2d(kernel_size=kernel_size_mp),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, norm_layer=None):\n",
    "        \"\"\"AlexNet style block without max pooling\"\"\"\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature extractor givven self.features and self.classifier\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204f191",
   "metadata": {},
   "source": [
    "### TODO 8:\n",
    "Prepare all components needed for training:\n",
    "1. Build your neural network with one type of regularization.\n",
    "2. Create DataLoaders for the training (and optionally validation) sets.\n",
    "3. Define the loss criterion.\n",
    "4. Define the optimizer and assign it the trainable parameters of your model.\n",
    "5. Print a summary of your network architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# your conde here\n",
    "import torch\n",
    "print(torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "modelBatchNorm = GarbageClassifier(num_classes=len(classes_names), norm_layer=MyBatchNorm2d)\n",
    "modelBatchNorm = modelBatchNorm.to(device)\n",
    "modelLayerNorm = GarbageClassifier(num_classes=len(classes_names), norm_layer=MyLayerNorm)\n",
    "modelLayerNorm = modelLayerNorm.to(device) \n",
    "\n",
    "lr = 0.0002\n",
    "batch_size = 128\n",
    "epoches = 10\n",
    "optimizerBatchNorm = torch.optim.Adam(modelBatchNorm.parameters(), lr=lr)\n",
    "optimizerLayerNorm = torch.optim.Adam(modelLayerNorm.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f1a77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d65289e",
   "metadata": {},
   "source": [
    "### TODO 9:\n",
    "Write a training loop to train your network for 10 epochs using the training set.\n",
    "- Track and print the training loss for each epoch.\n",
    "- After each epoch, compute and store both the loss and accuracy on the test set.\n",
    "- After training, plot both the training and test losses on the same graph to visualize the learning process.\n",
    "- Your model should achieve at least 75% accuracy on the test set.\n",
    "- Remember to set your model to training mode (`model.train()`) during training, and to evaluation mode (`model.eval()`) when computing metrics on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe7be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 76/76 [00:55<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.6616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 76/76 [02:24<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 1.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 76/76 [00:37<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 1.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 76/76 [00:39<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 76/76 [00:37<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.8464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 76/76 [00:35<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 0.7553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 76/76 [00:36<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 0.6306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 76/76 [00:56<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 0.5930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 76/76 [01:01<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 0.4644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 76/76 [01:05<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.4209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 76/76 [00:50<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Loss: 0.3715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 76/76 [00:36<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Loss: 0.3304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 76/76 [00:45<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Loss: 0.3155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 76/76 [00:48<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Loss: 0.2655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 76/76 [00:48<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Loss: 0.2563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 76/76 [00:48<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Loss: 0.2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 76/76 [00:36<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Loss: 0.1809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 76/76 [00:32<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Loss: 0.1610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 76/76 [00:33<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Loss: 0.1817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 76/76 [00:32<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 0.1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 76/76 [00:36<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Loss: 0.1531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 76/76 [00:39<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Loss: 0.1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 76/76 [00:37<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100], Loss: 0.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 76/76 [00:33<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Loss: 0.1240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 76/76 [00:34<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100], Loss: 0.1183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 76/76 [00:35<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Loss: 0.0888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 76/76 [00:32<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100], Loss: 0.0999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 76/76 [00:32<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Loss: 0.0816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 76/76 [00:38<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100], Loss: 0.0846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 76/76 [00:31<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 0.0812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 76/76 [00:33<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 0.0810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 76/76 [00:36<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.1068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 76/76 [00:35<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 76/76 [00:31<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 76/76 [00:31<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100], Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 76/76 [00:34<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Loss: 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 76/76 [00:32<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100], Loss: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 76/76 [00:32<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Loss: 0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 76/76 [00:32<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100], Loss: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 76/76 [00:31<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 76/76 [00:31<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100], Loss: 0.0558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 76/76 [00:33<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 76/76 [00:30<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100], Loss: 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 76/76 [00:36<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Loss: 0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100:  88%|████████▊ | 67/76 [00:39<00:05,  1.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: No samples in test set.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelBatchNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizerBatchNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m test_model(modelBatchNorm, test_loader, device, classes_names)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, test_loader, loss_fn, optimizer, device, epoches, classes_names)\u001b[39m\n\u001b[32m      4\u001b[39m model.train()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[32m      5\u001b[39m running_loss = \u001b[32m0.0\u001b[39m  \u001b[38;5;66;03m# Track the loss for each epoch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoches\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Move data to GPU\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Zero the gradient buffers\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[39m, in \u001b[36mdefault_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\datasets\\folder.py:262\u001b[39m, in \u001b[36mpil_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> Image.Image:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    263\u001b[39m         img = Image.open(f)\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# your code\n",
    "def train_model(model, train_loader, test_loader, loss_fn, optimizer, device, epoches, classes_names):\n",
    "    for epoch in range(epoches):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0  # Track the loss for each epoch\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epoches}\"):\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = loss_fn(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update model weights\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Print loss for the epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epoches}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "        import torch\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device, classes_names):\n",
    "        # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters\n",
    "    class_correct = [0 for _ in range(len(classes_names))]\n",
    "    class_total = [0 for _ in range(len(classes_names))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # Compare predictions to true labels\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    print(\"Per-class accuracy:\")\n",
    "    for i, class_name in enumerate(classes_names):\n",
    "        if class_total[i] > 0:\n",
    "            accuracy = 100 * class_correct[i] / class_total[i]\n",
    "            print(f\"Class {class_name}: {accuracy:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Class {class_name}: No samples in test set.\")\n",
    "\n",
    "train_model(modelBatchNorm, train_loader, test_loader, loss_fn, optimizerBatchNorm, device, epoches, classes_names)\n",
    "test_model(modelBatchNorm, test_loader, device, classes_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac5e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class accuracy:\n",
      "Class battery: 100.00%\n",
      "Class biological: 100.00%\n",
      "Class brown-glass: 50.00%\n",
      "Class cardboard: No samples in test set.\n",
      "Class clothes: 100.00%\n",
      "Class green-glass: No samples in test set.\n",
      "Class metal: 100.00%\n",
      "Class paper: 75.00%\n",
      "Class plastic: 100.00%\n",
      "Class shoes: 60.00%\n",
      "Class trash: No samples in test set.\n",
      "Class white-glass: 50.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model(modelBatchNorm, train_loader, test_loader, loss_fn, optimizerBatchNorm, device, epoches, classes_names)\n",
    "test_model(modelBatchNorm, test_loader, device, classes_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353319f",
   "metadata": {},
   "source": [
    "### TODO 10:\n",
    "Compute and report the accuracy of your trained model on the test set for each individual category (class).\n",
    "\n",
    "For example:\n",
    "* Class battery: 80%\n",
    "* Class biological: 71%\n",
    "* Class brown-glass: 70%\n",
    "* Class cardboard: 85%\n",
    "* Class clothes: 92%\n",
    "* Class green-glass: 88%\n",
    "* Class metal: 43%\n",
    "* Class paper: 54%\n",
    "* Class plastic: 39%\n",
    "* Class shoes: 71%\n",
    "* Class trash: 68%\n",
    "* Class white-glass: 55%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395350b4",
   "metadata": {},
   "source": [
    "### Repeat TODOs 8,9 and 10 with the other type of regularization\n",
    "\n",
    "Repeat the previous steps for preparing your model, DataLoaders, optimizer, and training loop, but this time using the alternative regularization layer. After training, compare the performance between the two regularization types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class accuracy:\n",
      "Class battery: 0.00%\n",
      "Class biological: 100.00%\n",
      "Class brown-glass: 100.00%\n",
      "Class cardboard: No samples in test set.\n",
      "Class clothes: 100.00%\n",
      "Class green-glass: No samples in test set.\n",
      "Class metal: 100.00%\n",
      "Class paper: 75.00%\n",
      "Class plastic: 100.00%\n",
      "Class shoes: 100.00%\n",
      "Class trash: No samples in test set.\n",
      "Class white-glass: 50.00%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (96) must match the size of tensor b (14) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m test_model(modelBatchNorm, test_loader, device, classes_names)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#train_model(modelLayerNorm, train_loader, test_loader, loss_fn, optimizerLayerNorm, device, epoches, classes_names)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelLayerNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(model, test_loader, device, classes_names)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[32m     35\u001b[39m     images, labels = images.to(device), labels.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m _, predicted = torch.max(outputs, \u001b[32m1\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Compare predictions to true labels\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mGarbageClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Forward pass through the feature extractor givven self.features and self.classifier\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.classifier(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mMyLayerNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     36\u001b[39m x_hat = (x - mean) / torch.sqrt(var + \u001b[38;5;28mself\u001b[39m.eps)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Scale and shift: apply learnable gamma and beta\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Broadcasting ensures gamma and beta are applied to the correct dimensions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_hat\u001b[49m + \u001b[38;5;28mself\u001b[39m.beta\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Output is normalized per sample, not per batch, which helps with stability\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# especially in recurrent networks or when batch sizes are small\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (96) must match the size of tensor b (14) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "test_model(modelBatchNorm, test_loader, device, classes_names)\n",
    "\n",
    "#train_model(modelLayerNorm, train_loader, test_loader, loss_fn, optimizerLayerNorm, device, epoches, classes_names)\n",
    "test_model(modelLayerNorm, test_loader, device, classes_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fde44",
   "metadata": {},
   "source": [
    "### Final Task: Summary and Analysis\n",
    "\n",
    "Write a summary of your work and the results you obtained. In 3–4 paragraphs, discuss your approach, key findings, and any challenges you encountered. Compare the performance of the two different regularization techniques you implemented, and suggest possible reasons for any differences you observed. Reflect on what you learned and what you might try differently in future experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de9f0c",
   "metadata": {},
   "source": [
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
