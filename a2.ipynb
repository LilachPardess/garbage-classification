{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5338f1f5",
   "metadata": {},
   "source": [
    "# Homework 2: Multiclass Classification with PyTorch\n",
    "\n",
    "In this assignment, you will build, train, and evaluate a neural network for multiclass classification using PyTorch.\n",
    "You will use the [Garbage dataset](https://www.kaggle.com/datasets/mostafaabla/garbage-classification).\n",
    "The goal is to gain hands-on experience with:\n",
    "- Dataset preparation \n",
    "- Building two  PyTorch models\n",
    "- Loss functions for multiclass\n",
    "- Training loop and evaluation\n",
    "- Visualize of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf82d3",
   "metadata": {},
   "source": [
    "## About Dataset\n",
    "### Context\n",
    "This dataset has 15,150 images from 12 different classes of household garbage; paper, cardboard, biological, metal, plastic, green-glass, brown-glass, white-glass, clothes, shoes, batteries, and trash.\n",
    "\n",
    "Garbage Recycling is a key aspect of preserving our environment. To make the recycling process possible/easier, the garbage must be sorted to groups that have similar recycling process. I found that most available data sets classify garbage into a few classes (2 to 6 classes at most). Having the ability to sort the household garbage into more classes can result in dramatically increasing the percentage of the recycled garbage.\n",
    "\n",
    "### Content\n",
    "An ideal setting for data collection would be to place a camera above a conveyor where the garbage comes one by one, so that the camera can capture real garbage images. But since such a setup is not feasible at the moment I collected most of the images in this dataset by web scraping, I tried to get images close to garbage images whenever possible, for example in biological garbage category I searched for rotten vegetables, rotten fruits and food remains, etc. However, for some classes such as clothes or shoes it was more difficult to get images of clothes or shoes from the garbage, so mostly it was images of normal clothes. Nevertheless, being able to classify the images of this data set to 12 classes can be a big step towards improving the recycling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f150469",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50767f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14c8798ef90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import kagglehub\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef162a",
   "metadata": {},
   "source": [
    "### Dwonload and prepare dataset from kagglehub\n",
    "`kagglehub.dataset_download` downloads and extracts Kaggle datasets to a local cache directory (usually under `~/.cache/kagglehub/datasets/`). It returns the path to the unzipped dataset, preserving the original folder structure as found on Kaggle, such as one subfolder per class for image datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is the structure of the downloaded content?**\n",
    "\n",
    "* Inside the returned directory (`path`), you will find the files and folders as originally organized on Kaggle.\n",
    "* For the **garbage classification** dataset, you typically get a folder like:\n",
    "\n",
    "  ```\n",
    "  garbage_classification/\n",
    "      cardboard/\n",
    "      glass/\n",
    "      metal/\n",
    "      paper/\n",
    "      plastic/\n",
    "      trash/\n",
    "      ...\n",
    "  ```\n",
    "\n",
    "  Each subfolder contains images belonging to that class (a classic structure for use with `torchvision.datasets.ImageFolder`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1087d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image transform: Compose(\n",
      "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Download the latest version of the Kaggle dataset to a local directory\n",
    "path = kagglehub.dataset_download(\"mostafaabla/garbage-classification\",)\n",
    "\n",
    "# Set the data directory to the location of the downloaded images\n",
    "data_dir = os.path.join(path, \"garbage_classification\")\n",
    "\n",
    "# Set the desired image size for resizing\n",
    "img_size = 64\n",
    "\n",
    "# Define the image transformations to apply to each image:\n",
    "# - Resize the image to (img_size, img_size)\n",
    "# - Convert the image to a PyTorch tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),  # Resize images\n",
    "    transforms.ToTensor()                     # Convert to tensor\n",
    "])  \n",
    "\n",
    "print(f\"image transform: {transform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed197d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpardess\\.cache\\kagglehub\\datasets\\mostafaabla\\garbage-classification\\versions\\1\\garbage_classification\n",
      "Number of images: 15515 number of classes: 12\n",
      "Class names: ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n"
     ]
    }
   ],
   "source": [
    "dataset  = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "classes_names = dataset.classes\n",
    "\n",
    "# dataset = ImageFolder(data_dir + '/Training', transform=transform)\n",
    "print(\"Number of images:\", len(dataset), \"number of classes:\", len(classes_names))\n",
    "print(\"Class names:\", classes_names)\n",
    "\n",
    "#fig, axes = plt.subplots(4, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f0e1e",
   "metadata": {},
   "source": [
    "### TODO 1:\n",
    "Create a 4Ã—3 subplot that displays one example image from each category in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b8327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your coimport matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_images(dataset):\n",
    "    fig, axes = plt.subplots(4, 3)\n",
    "    axes = axes.flatten() \n",
    "\n",
    "    shown = set()\n",
    "\n",
    "    for idx, class_name in enumerate(dataset.classes):\n",
    "        for img, label in dataset: # img is of size torch.Size([3, 64, 64])\n",
    "            if dataset.classes[label] == class_name and class_name not in shown:\n",
    "                axes[idx].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "                axes[idx].set_title(class_name)\n",
    "                axes[idx].axis('off')\n",
    "                shown.add(class_name)\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#TODO 1 delete\n",
    "def show_image(dataset, index):\n",
    "    fig, axes = plt.subplots(1, 1)\n",
    "\n",
    "    img, lable = dataset[index];\n",
    "    axes.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    axes.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2da23",
   "metadata": {},
   "source": [
    "### TODO 2:\n",
    "Shuffle the dataset and split it into training and validation sets, using 80% of the samples for training and 20% for validation. Make sure that the class distribution is preserved as much as possible in both splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15408b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def spit_dataset(dataset):\n",
    "    dataset_size = len(dataset) \n",
    "    # Create train/test split (80/20)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    test_size = len(dataset) - train_size\n",
    "    indices = torch.randperm(dataset_size)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def validate_distribution(dataset, title):\n",
    "    #TODO 1 imp\n",
    "    return;\n",
    "\n",
    "train_dataset, test_dataset = spit_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40025d8",
   "metadata": {},
   "source": [
    "### TODO 3:\n",
    "Visualize the class distribution in both the training and validation sets using a bar plot, so you can compare how well the splits represent the overall dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f391e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def show_distribution(subset, set_name):\n",
    "    print(\"Distuction of\", set_name, \": \")\n",
    "    class_sizes = [0] * len(classes_names)\n",
    "    for img, label in subset:\n",
    "        class_sizes[label] += 1\n",
    "\n",
    "    for idx in range(len(classes_names)):\n",
    "        percent = 100 * class_sizes[idx] / len(subset)\n",
    "        print(f\"Class '{classes_names[idx]}': {class_sizes[idx]} images ({percent:.2f}%)\")\n",
    "\n",
    "def get_class_counts(subset):\n",
    "    class_sizes = [0] * len(classes_names)\n",
    "    for img, label in subset:\n",
    "        class_sizes[label] += 1\n",
    "    return class_sizes\n",
    "\n",
    "def plot_class_distribution(train_dataset, test_dataset):\n",
    "    num_classes = len(classes_names)\n",
    "    train_counts = get_class_counts(train_dataset)\n",
    "    val_counts = get_class_counts(test_dataset)\n",
    "\n",
    "    # Convert counts to relative frequencies\n",
    "    train_total = sum(train_counts)\n",
    "    val_total = sum(val_counts)\n",
    "    train_rel = [count / train_total for count in train_counts]\n",
    "    val_rel = [count / val_total for count in val_counts]\n",
    "\n",
    "    x = range(num_classes)\n",
    "    width = 0.35  # width of the bars\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x, train_rel, width=width, label='Train')\n",
    "    plt.bar([i + width for i in x], val_rel, width=width, label='Validation')\n",
    "    plt.xticks([i + width/2 for i in x], classes_names, rotation=45)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Relative amount')\n",
    "    plt.title('Relative Class Distribution in Train and Validation Sets')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plot_class_distribution(train_dataset=train_dataset, test_dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9a21a",
   "metadata": {},
   "source": [
    "### TODO 4:\n",
    "Ensure that no single category accounts for more than 15% of the samples in the training set. If necessary, downsample the dominant classes. Then, visualize the new class distribution in the training set using a bar plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275c72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import random\n",
    "\n",
    "max_clothes = int(0.15 * len(train_dataset))\n",
    "clothes_class_idx = classes_names.index(\"clothes\")\n",
    "clothes_indices = [i for i, idx in enumerate(train_dataset.indices)\n",
    "                   if dataset.samples[idx][1] == clothes_class_idx]\n",
    "\n",
    "random.seed(42) \n",
    "selected_clothes_indices = random.sample(clothes_indices, max_clothes)\n",
    "other_indices = [i for i in range(len(train_dataset.indices)) if i not in clothes_indices]\n",
    "final_indices = [train_dataset.indices[i] for i in other_indices + selected_clothes_indices]\n",
    "\n",
    "train_dataset = Subset(dataset, final_indices)\n",
    "\n",
    "#plot_class_distribution(train_dataset=train_dataset, test_dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201f8e1",
   "metadata": {},
   "source": [
    "## Implementation of Regularization Layers\n",
    "\n",
    "Implement two regularization layers from scratch:\n",
    "1. `BatchNorm2d`\n",
    "2. `LayerNorm`\n",
    "\n",
    "Make sure that all trainable parameters (such as scale and shift) are properly registered as part of the computational graph, so they are optimized during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8360fb",
   "metadata": {},
   "source": [
    "### TODO 5:\n",
    "Implement the BatchNorm2d layer from scratch using only basic PyTorch components (such as `nn.Module` and tensor operations), without relying on `nn.BatchNorm2d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2f193d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features  # Number of feature channels (C in NCHW)\n",
    "        self.eps = eps  # Small value to avoid division by zero when normalizing\n",
    "        self.momentum = momentum  # Controls how quickly running stats are updated\n",
    "\n",
    "        # Learnable scale (gamma) and shift (beta) parameters, one per channel.\n",
    "        # These allow the network to undo normalization if needed, preserving representational power.\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "        # Running mean and variance are used during inference (model.eval()).\n",
    "        # They accumulate a moving average of the mean/variance seen during training.\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (N, C, H, W) - batch, channels, height, width\n",
    "\n",
    "        if self.training:\n",
    "            # During training, compute mean and variance for each channel over the batch and spatial dims.\n",
    "            # This normalizes each feature map (channel) so that its output has mean 0 and variance 1.\n",
    "            mean = x.mean(dim=(0, 2, 3), keepdim=True)  # Shape: (1, C, 1, 1)\n",
    "            var = x.var(dim=(0, 2, 3), unbiased=False, keepdim=True)  # Shape: (1, C, 1, 1)\n",
    "\n",
    "            # Update running statistics for use during inference.\n",
    "            # This helps the model make consistent predictions on unseen data (test/production).\n",
    "            # Detach is used so that these updates are not part of the gradient computation.\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.view(-1).detach()\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.view(-1).detach()\n",
    "        else:\n",
    "            # During inference, use the running mean/var instead of batch statistics.\n",
    "            # This ensures deterministic output and avoids dependence on batch size.\n",
    "            mean = self.running_mean.view(1, -1, 1, 1)\n",
    "            var = self.running_var.view(1, -1, 1, 1)\n",
    "\n",
    "        # Normalize: subtract mean and divide by stddev (add eps for stability).\n",
    "        # This reduces internal covariate shift, making training faster and more stable.\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # Scale and shift: allow the network to learn optimal mean/variance for each channel.\n",
    "        # This step ensures the transformation can represent the identity function if needed.\n",
    "        out = self.gamma.view(1, -1, 1, 1) * x_hat + self.beta.view(1, -1, 1, 1)\n",
    "\n",
    "        # The output is normalized, but can be rescaled and shifted as needed for learning.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72548b",
   "metadata": {},
   "source": [
    "### TODO 6:\n",
    "Implement the LayerNorm layer from scratch using only basic PyTorch components (such as `nn.Module` and tensor operations), without relying on `nn.LayerNorm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "853f8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            normalized_shape: input shape from an expected input of size\n",
    "                [*, normalized_shape[0], normalized_shape[1], ..., normalized_shape[-1]]\n",
    "                Typically, for images, this is the number of channels (C), height (H), and width (W).\n",
    "            eps: a value added to the denominator for numerical stability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Store the shape over which normalization is performed\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = tuple(normalized_shape)\n",
    "        self.eps = eps\n",
    "\n",
    "        # Learnable parameters for scaling (gamma) and shifting (beta)\n",
    "        # These allow the network to undo normalization if needed\n",
    "        self.gamma = nn.Parameter(torch.ones(self.normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute mean and variance over the last N dimensions (those in normalized_shape)\n",
    "        # This is different from BatchNorm, which normalizes over batch and spatial dims\n",
    "        dims = tuple(-i for i in range(1, len(self.normalized_shape)+1))\n",
    "        mean = x.mean(dim=dims, keepdim=True)  # Mean over normalized_shape\n",
    "        var = x.var(dim=dims, unbiased=False, keepdim=True)  # Variance over normalized_shape\n",
    "\n",
    "        # Normalize: subtract mean and divide by stddev (add eps for stability)\n",
    "        # This ensures each sample (not batch) has zero mean and unit variance over the normalized_shape\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # Scale and shift: apply learnable gamma and beta\n",
    "        # Broadcasting ensures gamma and beta are applied to the correct dimensions\n",
    "        out = self.gamma * x_hat + self.beta\n",
    "\n",
    "        # Output is normalized per sample, not per batch, which helps with stability\n",
    "        # especially in recurrent networks or when batch sizes are small\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd3273",
   "metadata": {},
   "source": [
    "## Traning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855054a3",
   "metadata": {},
   "source": [
    "### TODO 7:\n",
    "Complete the `GarbageClassifier` neural network by designing and implementing an architecture of your choice.  \n",
    "Make use of the provided `_block` and `_block_mp` building blocks as you see fit.  \n",
    "Allow the regularization type (e.g., `BatchNorm2d` or `LayerNorm`) to be specified from outside the class, so you can later compare the results between the two types of regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d851b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, norm_layer):\n",
    "        # Fix: Remove norm_layer from super() call\n",
    "        super(GarbageClassifier, self).__init__()\n",
    "        \n",
    "        # Set default normalization layer if none provided\n",
    "        if norm_layer is None:\n",
    "            norm_layer = MyBatchNorm2d \n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            self._block_mp(3, 96, kernel_size=11, stride=4, padding=0, norm_layer=norm_layer),\n",
    "            self._block_mp(96, 256, kernel_size=5, stride=1, padding=2, norm_layer=norm_layer),\n",
    "            self._block(256, 384, kernel_size=3, stride=1, padding=1, norm_layer=norm_layer),\n",
    "            self._block(384, 384, kernel_size=3, stride=1, padding=1, norm_layer=norm_layer),\n",
    "            self._block_mp(384, 256, kernel_size=3, stride=1, padding=1, norm_layer=norm_layer),\n",
    "        )   \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 1 * 1, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "       \n",
    "    def _block_mp(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, norm_layer=None, kernel_size_mp=2):\n",
    "        \"\"\"AlexNet style block with max pooling\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            norm_layer(out_channels), # Normalization layer that you have implemented\n",
    "            nn.MaxPool2d(kernel_size=kernel_size_mp),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, norm_layer=None):\n",
    "        \"\"\"AlexNet style block without max pooling\"\"\"\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the feature extractor givven self.features and self.classifier\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204f191",
   "metadata": {},
   "source": [
    "### TODO 8:\n",
    "Prepare all components needed for training:\n",
    "1. Build your neural network with one type of regularization.\n",
    "2. Create DataLoaders for the training (and optionally validation) sets.\n",
    "3. Define the loss criterion.\n",
    "4. Define the optimizer and assign it the trainable parameters of your model.\n",
    "5. Print a summary of your network architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# your conde here\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: \", device)\n",
    "\n",
    "modelBatchNorm = GarbageClassifier(num_classes=len(classes_names), norm_layer=MyBatchNorm2d)\n",
    "modelBatchNorm = modelBatchNorm.to(device)\n",
    "modelLayerNorm = GarbageClassifier(num_classes=len(classes_names), norm_layer=MyLayerNorm)\n",
    "modelLayerNorm = modelLayerNorm.to(device) \n",
    "\n",
    "lr = 0.0002\n",
    "batch_size = 128\n",
    "epoches = 10\n",
    "optimizerBatchNorm = torch.optim.Adam(modelBatchNorm.parameters(), lr=lr)\n",
    "optimizerLayerNorm = torch.optim.Adam(modelLayerNorm.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f1a77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d65289e",
   "metadata": {},
   "source": [
    "### TODO 9:\n",
    "Write a training loop to train your network for 10 epochs using the training set.\n",
    "- Track and print the training loss for each epoch.\n",
    "- After each epoch, compute and store both the loss and accuracy on the test set.\n",
    "- After training, plot both the training and test losses on the same graph to visualize the learning process.\n",
    "- Your model should achieve at least 75% accuracy on the test set.\n",
    "- Remember to set your model to training mode (`model.train()`) during training, and to evaluation mode (`model.eval()`) when computing metrics on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3fe7be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [02:16<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:33<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.2216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:32<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 1.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.9211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:35<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.8295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:37<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.7329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:43<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.6317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:37<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.5768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:36<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.4943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:39<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.4509\n",
      "Per-class accuracy:\n",
      "Class battery: 58.85%\n",
      "Class biological: 69.83%\n",
      "Class brown-glass: 83.97%\n",
      "Class cardboard: 75.47%\n",
      "Class clothes: 80.57%\n",
      "Class green-glass: 69.23%\n",
      "Class metal: 27.40%\n",
      "Class paper: 72.81%\n",
      "Class plastic: 21.38%\n",
      "Class shoes: 80.05%\n",
      "Class trash: 59.71%\n",
      "Class white-glass: 45.14%\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "def train_model(model, train_loader, test_loader, loss_fn, optimizer, device, epoches, classes_names):\n",
    "    for epoch in range(epoches):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0  # Track the loss for each epoch\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epoches}\"):\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradient buffers\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = loss_fn(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update model weights\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Print loss for the epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epoches}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "        import torch\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device, classes_names):\n",
    "        # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters\n",
    "    class_correct = [0 for _ in range(len(classes_names))]\n",
    "    class_total = [0 for _ in range(len(classes_names))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # Compare predictions to true labels\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    print(\"Per-class accuracy:\")\n",
    "    for i, class_name in enumerate(classes_names):\n",
    "        if class_total[i] > 0:\n",
    "            accuracy = 100 * class_correct[i] / class_total[i]\n",
    "            print(f\"Class {class_name}: {accuracy:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Class {class_name}: No samples in test set.\")\n",
    "\n",
    "train_model(modelBatchNorm, train_loader, test_loader, loss_fn, optimizerBatchNorm, device, epoches, classes_names)\n",
    "test_model(modelBatchNorm, test_loader, device, classes_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eac5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model(modelBatchNorm, test_loader, device, classes_names)\n",
    "\n",
    "#train_model(modelBatchNorm, train_loader, test_loader, loss_fn, optimizerBatchNorm, device, epoches, classes_names)\n",
    "#test_model(modelBatchNorm, test_loader, device, classes_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353319f",
   "metadata": {},
   "source": [
    "### TODO 10:\n",
    "Compute and report the accuracy of your trained model on the test set for each individual category (class).\n",
    "\n",
    "For example:\n",
    "* Class battery: 80%\n",
    "* Class biological: 71%\n",
    "* Class brown-glass: 70%\n",
    "* Class cardboard: 85%\n",
    "* Class clothes: 92%\n",
    "* Class green-glass: 88%\n",
    "* Class metal: 43%\n",
    "* Class paper: 54%\n",
    "* Class plastic: 39%\n",
    "* Class shoes: 71%\n",
    "* Class trash: 68%\n",
    "* Class white-glass: 55%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395350b4",
   "metadata": {},
   "source": [
    "### Repeat TODOs 8,9 and 10 with the other type of regularization\n",
    "\n",
    "Repeat the previous steps for preparing your model, DataLoaders, optimizer, and training loop, but this time using the alternative regularization layer. After training, compare the performance between the two regularization types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6690063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model(modelBatchNorm, test_loader, device, classes_names)\n",
    "\n",
    "#train_model(modelLayerNorm, train_loader, test_loader, loss_fn, optimizerLayerNorm, device, epoches, classes_names)\n",
    "#test_model(modelLayerNorm, test_loader, device, classes_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fde44",
   "metadata": {},
   "source": [
    "### Final Task: Summary and Analysis\n",
    "\n",
    "Write a summary of your work and the results you obtained. In 3â€“4 paragraphs, discuss your approach, key findings, and any challenges you encountered. Compare the performance of the two different regularization techniques you implemented, and suggest possible reasons for any differences you observed. Reflect on what you learned and what you might try differently in future experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de9f0c",
   "metadata": {},
   "source": [
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
